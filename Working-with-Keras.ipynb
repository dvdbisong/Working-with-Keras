{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keras\n",
    "\n",
    "Table of contents:\n",
    "\n",
    "- [Notebook Instance on GCP AI Platform](#notebook-instance)\n",
    "- [The Anatomy of a Keras Program](#the-anatomy-of-a-keras-program)\n",
    "- [Multilayer Perceptron (MLP) with Keras](#multilayer-perceptron-mlp-with-keras)\n",
    "- [Using the Dataset API with tf.keras](#using-the-dataset-api-with-tfkeras)\n",
    "- [TensorBoard with Keras](#tensorboard-with-keras)\n",
    "- [Checkpointing to Select Best Models](#checkpointing-to-select-best-models)\n",
    "- [Convolutional Neural Networks (CNNs) with Keras](#convolutional-neural-networks-cnns-with-keras)\n",
    "- [Recurrent Neural Networks (RNNs) with Keras](#recurrent-neural-networks-rnns-with-keras)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"#notebook-instance\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook Instance on GCP AI Platform\n",
    "\n",
    "**Open Notebook Instance**\n",
    "![Open Notebook Instance.](img/gcp-notebooks.png \"Open Notebook Instance\")\n",
    "\n",
    "**Notebook Instances Dashboard**\n",
    "![Notebook Instances Dashboard.](img/gcp-notebook-instances-dashboard.png \"Notebook Instances Dashboard\")\n",
    "\n",
    "**Start a New Instance**\n",
    "![Start a New Instance.](img/gcp-new-instance.png \"Start a New Instance\")\n",
    "![Start a New Instance.](img/gcp-new-instance-2.png \"Start a New Instance\")\n",
    "\n",
    "**Open Jupyterlab**\n",
    "![Open JupyterLab.](img/gcp-open-jupyterlab.png \"Open JupyterLab\")\n",
    "![Open JupyterLab.](img/gcp-open-jupyterlab-2.png \"Open JupyterLab\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"#the-anatomy-of-a-keras-program\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Anatomy of a Keras Program\n",
    "* The **Keras Model** is the core of a Keras programme.\n",
    "* A Model is:\n",
    "  * constructed,\n",
    "  * compiled, then \n",
    "  * trained and evaluated using their respective training and evaluation datasets.\n",
    "  \n",
    "* Upon satisfactory evaluation, the model is used to make predictions on previously unseen data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Program flow for modeling with Keras.](img/keras-program.png \"Anatomy of a Keras Program\")\n",
    "**Program flow for modeling with Keras.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Keras API\n",
    "A Keras Model can be constructed using:\n",
    "* The Sequential API `tf.keras.Sequential` (simple API for creating a linear stack of Neural Networks layers), or\n",
    "* The Keras Functional API which defines a model instance `tf.keras.Model` for more complex network graphs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multilayer Perceptron (MLP) with Keras\n",
    "We'll go through the following steps:\n",
    "- Import and transform the dataset.\n",
    "- Build and Compile the Model\n",
    "- Train the data using `Model.fit()`\n",
    "- Evaluate the Model using `Model.evaluate()`\n",
    "- Predict on unseen data using `Model.predict()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Dataset\n",
    "* The dataset used for this example is the Fashion-MNIST database of fashion articles.\n",
    "* It is similar to the MNIST handwriting dataset.\n",
    "* It contains 60,000 28x28 pixel grayscale images.\n",
    "This dataset and more can be found in the `tf.keras.datasets` package."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess\n",
    "**Flatten from 2-dimensional shape to 1-dimension**  \n",
    "![Flattened Image](img/flatten-image.png \"Flattened Image\")\n",
    "\n",
    "**Scale pixels from 0 to 255 to be within the range of 0 to 1.**  \n",
    "![Scaled pixels](img/scale-image.png \"Scaled pixels\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "# import dataset\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.fashion_mnist.load_data()\n",
    "\n",
    "# flatten the 28*28 pixel images into one long 784 pixel vector\n",
    "x_train = np.reshape(x_train, (-1, 784)).astype('float32')\n",
    "x_test = np.reshape(x_test, (-1, 784)).astype('float32')\n",
    "\n",
    "# scale dataset from 0 -> 255 to 0 -> 1\n",
    "x_train /= 255\n",
    "x_test /= 255\n",
    "\n",
    "# one-hot encode targets\n",
    "y_train = tf.keras.utils.to_categorical(y_train)\n",
    "y_test = tf.keras.utils.to_categorical(y_test)\n",
    " \n",
    "# create the model\n",
    "def model_fn():\n",
    "    model = tf.keras.Sequential()\n",
    "    # Adds a densely-connected layer with 256 units to the model:\n",
    "    model.add(tf.keras.layers.Dense(256, activation='relu', input_dim=784))\n",
    "    # Add Dropout layer\n",
    "    model.add(tf.keras.layers.Dropout(0.2))\n",
    "    # Add another densely-connected layer with 64 units:\n",
    "    model.add(tf.keras.layers.Dense(64, activation='relu'))\n",
    "    # Add a softmax layer with 10 output units:\n",
    "    model.add(tf.keras.layers.Dense(10, activation='softmax'))\n",
    "    \n",
    "    # compile the model\n",
    "    model.compile(optimizer=tf.train.AdamOptimizer(0.001),\n",
    "                  loss='categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# build model\n",
    "model = model_fn()\n",
    "\n",
    "# train the model\n",
    "model.fit(x_train, y_train, epochs=10,\n",
    "          batch_size=100, verbose=1,\n",
    "          validation_data=(x_test, y_test))\n",
    "\n",
    "# evaluate the model\n",
    "score = model.evaluate(x_test, y_test, batch_size=100)\n",
    "print('Test loss: {:.2f} \\nTest accuracy: {:.2f}%'.format(score[0], score[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"#using-the-dataset-api-with-tfkeras\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using the Dataset API with tf.keras\n",
    "- The Dataset API is the preferred mechanism for feeding data as it can easily scale to handle very large datasets and better utilize the machine resources.\n",
    "- The data pipeline is constructed using the Dataset API and the Model is constructed using the **Keras Functional API**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "# import dataset\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.fashion_mnist.load_data()\n",
    "\n",
    "# flatten the 28*28 pixel images into one long 784 pixel vector\n",
    "x_train = np.reshape(x_train, (-1, 784)).astype('float32')\n",
    "x_test = np.reshape(x_test, (-1, 784)).astype('float32')\n",
    "\n",
    "# scale dataset from 0 -> 255 to 0 -> 1\n",
    "x_train /= 255\n",
    "x_test /= 255\n",
    "\n",
    "# one-hot encode targets\n",
    "y_train = tf.keras.utils.to_categorical(y_train)\n",
    "y_test = tf.keras.utils.to_categorical(y_test)\n",
    "\n",
    "# create dataset pipeline\n",
    "def input_fn(features, labels, batch_size, training=True):\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((features, labels))\n",
    "    if training:\n",
    "        dataset = dataset.shuffle(buffer_size=1000)\n",
    "        dataset = dataset.repeat()\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    iterator = dataset.make_one_shot_iterator()\n",
    "    features, labels = iterator.get_next()    \n",
    "    return features, labels\n",
    "\n",
    "# parameters\n",
    "batch_size = 100\n",
    "training_steps_per_epoch = int(np.ceil(x_train.shape[0] / float(batch_size)))  # ==> 600\n",
    "eval_steps_per_epoch = int(np.ceil(x_test.shape[0] / float(batch_size)))  # ==> 100\n",
    "epochs = 10\n",
    "\n",
    "# create the model\n",
    "def model_fn(input_fn):\n",
    "    \n",
    "    (features, labels) = input_fn\n",
    "    \n",
    "    # Model input\n",
    "    model_input = tf.keras.layers.Input(tensor=features)\n",
    "    # Adds a densely-connected layer with 256 units to the model:\n",
    "    x = tf.keras.layers.Dense(256, activation='relu', input_dim=784)(model_input)\n",
    "    # Add Dropout layer:\n",
    "    x = tf.keras.layers.Dropout(0.2)(x)\n",
    "    # Add another densely-connected layer with 64 units:\n",
    "    x = tf.keras.layers.Dense(64, activation='relu')(x)\n",
    "    # Add a softmax layer with 10 output units:\n",
    "    predictions = tf.keras.layers.Dense(10, activation='softmax')(x)\n",
    "    \n",
    "    # the model\n",
    "    model = tf.keras.Model(inputs=model_input, outputs=predictions)\n",
    "    \n",
    "    # compile the model\n",
    "    model.compile(optimizer='adam',\n",
    "                  loss='categorical_crossentropy',\n",
    "                  metrics=['accuracy'],\n",
    "                  target_tensors=[tf.cast(labels,tf.float32)])\n",
    "    return model\n",
    "\n",
    "# build train model\n",
    "model = model_fn(input_fn(x_train, y_train, batch_size=batch_size, training=True))\n",
    "\n",
    "# print train model summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the model\n",
    "history = model.fit(epochs=epochs,\n",
    "                    steps_per_epoch=training_steps_per_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store trained model weights\n",
    "model.save_weights('/tmp/mlp_weight.h5')\n",
    "\n",
    "# build evaluation model\n",
    "eval_model = model_fn(input_fn(x_test, y_test, batch_size=batch_size, training=False))\n",
    "# load saved weights to evaluation model\n",
    "eval_model.load_weights('/tmp/mlp_weight.h5')\n",
    "\n",
    "# evaluate the model\n",
    "score = eval_model.evaluate(steps=eval_steps_per_epoch)\n",
    "print('Test loss: {:.2f} \\nTest accuracy: {:.2f}%'.format(score[0], score[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"#tensorboard-with-keras\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TensorBoard with Keras\n",
    "To visualize models with TensorBoard, attach a TensorBoard callback `tf.keras.callbacks.TensorBoard()` to the `model.fit()` method before training the model. The model graph, scalars, histograms, and other metrics are stored as event files in the log directory.\n",
    "\n",
    "For this example, we'll build an MLP MNIST model and visualize it with TensorBoard."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "# import dataset\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "\n",
    "# flatten the 28*28 pixel images into one long 784 pixel vector\n",
    "x_train = np.reshape(x_train, (-1, 784)).astype('float32')\n",
    "x_test = np.reshape(x_test, (-1, 784)).astype('float32')\n",
    "\n",
    "# scale dataset from 0 -> 255 to 0 -> 1\n",
    "x_train /= 255\n",
    "x_test /= 255\n",
    "\n",
    "# one-hot encode targets\n",
    "y_train = tf.keras.utils.to_categorical(y_train)\n",
    "y_test = tf.keras.utils.to_categorical(y_test)\n",
    " \n",
    "# create the model\n",
    "def model_fn():\n",
    "    inputs = tf.keras.Input(shape=(784,))\n",
    "    x = tf.keras.layers.Dense(128, activation='relu', input_dim=784)(inputs)\n",
    "    x = tf.keras.layers.Dropout(0.2)(x)\n",
    "    x = tf.keras.layers.Dense(64, activation='relu')(x)\n",
    "    prediction = tf.keras.layers.Dense(10, activation='softmax')(x)\n",
    "    \n",
    "    model = tf.keras.Model(inputs=inputs, outputs=prediction)\n",
    "    \n",
    "    # compile the model\n",
    "    model.compile(optimizer='adam',\n",
    "                  loss='categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# build model\n",
    "model = model_fn()\n",
    "\n",
    "# checkpointing\n",
    "best_model = tf.keras.callbacks.ModelCheckpoint('./tmp/mnist_weights.h5', monitor='val_acc',\n",
    "                                                verbose=1, save_best_only=True, mode='max')\n",
    "\n",
    "# tensorboard\n",
    "tensorboard = tf.keras.callbacks.TensorBoard(log_dir='./tmp/logs_mnist_keras',\n",
    "                                             histogram_freq=0, write_graph=True,\n",
    "                                             write_images=True)\n",
    "\n",
    "callbacks = [best_model, tensorboard]\n",
    "\n",
    "# train the model\n",
    "history = model.fit(x_train, y_train, epochs=10,\n",
    "                    batch_size=100, verbose=1,\n",
    "                    validation_data=(x_test, y_test),\n",
    "                    callbacks=callbacks)\n",
    "\n",
    "# evaluate the model\n",
    "score = model.evaluate(x_test, y_test, batch_size=100)\n",
    "print('Test loss: {:.2f} \\nTest accuracy: {:.2f}%'.format(score[0], score[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# execute the command to run TensorBoard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!tensorboard --logdir tmp/logs_mnist_keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"#checkpointing-to-select-best-models\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checkpointing to Select Best Models\n",
    "- Checkpointing makes it possible to save the weights of the neural network model when there is an increase in the validation accuracy metric.\n",
    "- This is achieved in Keras using the method `tf.keras.callbacks.ModelCheckpoint()`.\n",
    "- The saved weights can then be loaded back into the model and used to make predictions.\n",
    "\n",
    "Using the MNIST dataset, we'll build a model that saves the weights to file only when there is an improvement in the validation set performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "# import dataset\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "\n",
    "# flatten the 28*28 pixel images into one long 784 pixel vector\n",
    "x_train = np.reshape(x_train, (-1, 784)).astype('float32')\n",
    "x_test = np.reshape(x_test, (-1, 784)).astype('float32')\n",
    "\n",
    "# scale dataset from 0 -> 255 to 0 -> 1\n",
    "x_train /= 255\n",
    "x_test /= 255\n",
    "\n",
    "# one-hot encode targets\n",
    "y_train = tf.keras.utils.to_categorical(y_train)\n",
    "y_test = tf.keras.utils.to_categorical(y_test)\n",
    " \n",
    "# create the model\n",
    "def model_fn():\n",
    "    inputs = tf.keras.Input(shape=(784,))\n",
    "    x = tf.keras.layers.Dense(128, activation='relu', input_dim=784)(inputs)\n",
    "    x = tf.keras.layers.Dropout(0.2)(x)\n",
    "    x = tf.keras.layers.Dense(64, activation='relu')(x)\n",
    "    prediction = tf.keras.layers.Dense(10, activation='softmax')(x)\n",
    "    \n",
    "    model = tf.keras.Model(inputs=inputs, outputs=prediction)\n",
    "    \n",
    "    # compile the model\n",
    "    model.compile(optimizer='adam',\n",
    "                  loss='categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# build model\n",
    "model = model_fn()\n",
    "\n",
    "# checkpointing\n",
    "checkpoint = tf.keras.callbacks.ModelCheckpoint('mnist_weights.h5', monitor='val_acc',\n",
    "                                                verbose=1, save_best_only=True, mode='max')\n",
    "callbacks = [checkpoint]\n",
    "\n",
    "# train the model\n",
    "history = model.fit(x_train, y_train, epochs=10,\n",
    "                    batch_size=100, verbose=1,\n",
    "                    validation_data=(x_test, y_test),\n",
    "                    callbacks=callbacks)\n",
    "\n",
    "# evaluate the model\n",
    "score = model.evaluate(x_test, y_test, batch_size=100)\n",
    "print('Test loss: {:.2f} \\nTest accuracy: {:.2f}%'.format(score[0], score[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"#convolutional-neural-networks-cnns-with-keras\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional Neural Networks (CNNs) with Keras\n",
    "In this section, we use Keras to implement a Convolutional Neural Network. The network architecture is similar to the [Krizhevsky architecture](https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf) and consists of the following layers:\n",
    "- Convolution layer: kernel_size => [5 x 5]\n",
    "- Convolution layer: kernel_size => [5 x 5]\n",
    "- Batch Normalization layer\n",
    "- Convolution layer: kernel_size => [5 x 5]\n",
    "- Max pooling: pool size => [2 x 2]\n",
    "- Convolution layer: kernel_size => [5 x 5]\n",
    "- Convolution layer: kernel_size => [5 x 5]\n",
    "- Batch Normalization layer\n",
    "- Max pooling: pool size => [2 x 2]\n",
    "- Convolution layer: kernel_size => [5 x 5]\n",
    "- Convolution layer: kernel_size => [5 x 5]\n",
    "- Convolution layer: kernel_size => [5 x 5]\n",
    "- Max pooling: pool size => [2 x 2]\n",
    "- Dropout layer\n",
    "- Dense Layer: units => [512]\n",
    "- Dense Layer: units => [256]\n",
    "- Dropout layer\n",
    "- Dense Layer: units => [10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset\n",
    "* The dataset used for this example is the Cifar10 image dataset.\n",
    "* It consists of 60000 32x32 colour images in 10 classes, with 6000 images per class.\n",
    "* There are 50000 training images and 10000 test images.\n",
    "* The 10 classes are: [airplane, automobile, bird, cat, deer, dog, frog, horse, ship, truck]\n",
    "\n",
    "This dataset and more can be found in the `tf.keras.datasets` package."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import dataset\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.cifar10.load_data()\n",
    "\n",
    "# change datatype to float\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "\n",
    "# scale the dataset from 0 -> 255 to 0 -> 1\n",
    "x_train /= 255\n",
    "x_test /= 255\n",
    "\n",
    "# one-hot encode targets\n",
    "y_train = tf.keras.utils.to_categorical(y_train)\n",
    "y_test = tf.keras.utils.to_categorical(y_test)\n",
    "\n",
    "# create dataset pipeline\n",
    "def input_fn(features, labels, batch_size, training=True):\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((features, labels))\n",
    "    if training:\n",
    "        dataset = dataset.shuffle(buffer_size=1000)\n",
    "        dataset = dataset.repeat()\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    iterator = dataset.make_one_shot_iterator()\n",
    "    features, labels = iterator.get_next()    \n",
    "    return features, labels\n",
    "\n",
    "# parameters\n",
    "batch_size = 100\n",
    "training_steps_per_epoch = int(np.ceil(x_train.shape[0] / float(batch_size)))  # ==> 600\n",
    "eval_steps_per_epoch = int(np.ceil(x_test.shape[0] / float(batch_size)))  # ==> 100\n",
    "epochs = 10\n",
    "\n",
    "# create the model\n",
    "def model_fn(input_fn):\n",
    "    \n",
    "    (features, labels) = input_fn\n",
    "    \n",
    "    # Model input\n",
    "    model_input = tf.keras.layers.Input(tensor=features)\n",
    "    x = tf.keras.layers.Conv2D(64, (5, 5), padding='same', activation='relu')(model_input)\n",
    "    x = tf.keras.layers.Conv2D(64, (5, 5), padding='same', activation='relu')(x)\n",
    "    x = tf.keras.layers.BatchNormalization()(x)\n",
    "    x = tf.keras.layers.Conv2D(64, (5, 5), padding='same', activation='relu')(x)\n",
    "    x = tf.keras.layers.MaxPooling2D(pool_size=(2, 2), strides=2, padding='same')(x)\n",
    "    x = tf.keras.layers.Conv2D(64, (5, 5), padding='same', activation='relu')(x)\n",
    "    x = tf.keras.layers.Conv2D(64, (5, 5), padding='same', activation='relu')(x)\n",
    "    x = tf.keras.layers.BatchNormalization()(x)\n",
    "    x = tf.keras.layers.MaxPooling2D(pool_size=(2, 2), strides=2, padding='same')(x)\n",
    "    x = tf.keras.layers.Conv2D(32, (3, 3), padding='same', activation='relu')(x)\n",
    "    x = tf.keras.layers.Conv2D(32, (3, 3), padding='same', activation='relu')(x)\n",
    "    x = tf.keras.layers.Conv2D(32, (3, 3), padding='same', activation='relu')(x)\n",
    "    x = tf.keras.layers.MaxPooling2D(pool_size=(2, 2), strides=2, padding='same')(x)\n",
    "    x = tf.keras.layers.Dropout(0.3)(x)\n",
    "    x = tf.keras.layers.Flatten()(x)\n",
    "    x = tf.keras.layers.Dense(512, activation='relu')(x)\n",
    "    x = tf.keras.layers.Dense(256, activation='relu')(x)\n",
    "    x = tf.keras.layers.Dropout(0.5)(x)\n",
    "    output = tf.keras.layers.Dense(10, activation='softmax')(x)\n",
    "    \n",
    "    # the model\n",
    "    model = tf.keras.Model(inputs=model_input, outputs=output)\n",
    "    \n",
    "    # compile the model\n",
    "    model.compile(optimizer=tf.keras.optimizers.Nadam(),\n",
    "                  loss='categorical_crossentropy',\n",
    "                  metrics=['accuracy'],\n",
    "                  target_tensors=[tf.cast(labels,tf.float32)])\n",
    "    return model\n",
    "\n",
    "# build train model\n",
    "model = model_fn(input_fn(x_train, y_train, batch_size=batch_size, training=True))\n",
    "\n",
    "# print train model summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the model\n",
    "history = model.fit(epochs=epochs,\n",
    "                    steps_per_epoch=training_steps_per_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store trained model weights\n",
    "model.save_weights('./tmp/cnn_weight.h5')\n",
    "\n",
    "# build evaluation model\n",
    "eval_model = model_fn(input_fn(x_test, y_test, batch_size=batch_size, training=False))\n",
    "eval_model.load_weights('./tmp/cnn_weight.h5')\n",
    "\n",
    "# evaluate the model\n",
    "score = eval_model.evaluate(steps=eval_steps_per_epoch)\n",
    "print('Test loss: {:.2f} \\nTest accuracy: {:.2f}%'.format(score[0], score[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"#recurrent-neural-networks-rnns-with-keras\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recurrent Neural Networks (RNNs) with Keras\n",
    "This section uses the Nigeria power consumption univariate timeseries dataset to implement a LSTM Recurrent Neural Network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# data url\n",
    "url = \"https://raw.githubusercontent.com/dvdbisong/gcp-learningmodels-book/master/Chapter_12/nigeria-power-consumption.csv\"\n",
    "\n",
    "# load data\n",
    "parse_date = lambda dates: pd.datetime.strptime(dates, '%d-%m')\n",
    "data = pd.read_csv(url, parse_dates=['Month'], index_col='Month',\n",
    "                   date_parser=parse_date,\n",
    "                   engine='python', skipfooter=2)\n",
    "\n",
    "# print column name\n",
    "data.columns\n",
    "\n",
    "# change column names\n",
    "data.rename(columns={'Nigeria power consumption': 'power-consumption'},\n",
    "            inplace=True)\n",
    "\n",
    "# split in training and evaluation set\n",
    "data_train, data_eval = train_test_split(data, test_size=0.2, shuffle=False)\n",
    "\n",
    "# MinMaxScaler - center ans scale the dataset\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "data_train = scaler.fit_transform(data_train)\n",
    "data_eval = scaler.fit_transform(data_eval)\n",
    "\n",
    "# adjust univariate data for timeseries prediction\n",
    "def convert_to_sequences(data, sequence, is_target=False):\n",
    "    temp_df = []\n",
    "    for i in range(len(data) - sequence):\n",
    "        if is_target:\n",
    "            temp_df.append(data[(i+1): (i+1) + sequence])\n",
    "        else:\n",
    "            temp_df.append(data[i: i + sequence])\n",
    "    return np.array(temp_df)\n",
    "\n",
    "# parameters\n",
    "time_steps = 20\n",
    "inputs = 1\n",
    "neurons = 100\n",
    "outputs = 1\n",
    "batch_size = 32\n",
    "\n",
    "# create training and testing data\n",
    "train_x = convert_to_sequences(data_train, time_steps, is_target=False)\n",
    "train_y = convert_to_sequences(data_train, time_steps, is_target=True)\n",
    "\n",
    "eval_x = convert_to_sequences(data_eval, time_steps, is_target=False)\n",
    "eval_y = convert_to_sequences(data_eval, time_steps, is_target=True)\n",
    "\n",
    "# Build model\n",
    "model = tf.keras.Sequential()\n",
    "model.add(tf.keras.layers.LSTM(128, dropout=0.2, recurrent_dropout=0.2,\n",
    "                               input_shape=train_x.shape[1:],\n",
    "                               return_sequences=True))\n",
    "model.add(tf.keras.layers.Dense(1))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='mean_squared_error',\n",
    "              optimizer='adam',\n",
    "              metrics=['mse'])\n",
    "\n",
    "# print model summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "history = model.fit(train_x, train_y,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=20, shuffle=False,\n",
    "                    validation_data=(eval_x, eval_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, mse = model.evaluate(eval_x, eval_y, batch_size=batch_size)\n",
    "print('Test loss: {:.4f}'.format(loss))\n",
    "print('Test mse: {:.4f}'.format(mse))\n",
    "\n",
    "# predict\n",
    "y_pred = model.predict(eval_x)\n",
    "\n",
    "# plot predicted sequence\n",
    "plt.title(\"Model Testing\", fontsize=12)\n",
    "plt.plot(eval_x[0,:,0], \"b--\", markersize=10, label=\"training instance\")\n",
    "plt.plot(eval_y[0,:,0], \"g--\", markersize=10, label=\"targets\")\n",
    "plt.plot(y_pred[0,:,0], \"r--\", markersize=10, label=\"model prediction\")\n",
    "plt.legend(loc=\"upper left\")\n",
    "plt.xlabel(\"Time\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf2",
   "language": "python",
   "name": "tf2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
